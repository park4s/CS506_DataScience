{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector Spaces, Distance, and Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'slideUtilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-586baa965e7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mslideUtilities\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'slideUtilities'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import slideUtilities as sl\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "from datetime import datetime\n",
    "from IPython.display import Image\n",
    "from IPython.display import display_html\n",
    "from IPython.display import display\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    " .container.slides .celltoolbar, .container.slides .hide-in-slideshow {\n",
    "    display: None ! important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "%Set up useful MathJax (Latex) macros.\n",
    "%See http://docs.mathjax.org/en/latest/tex.html#defining-tex-macros\n",
    "%These are for use in the slideshow\n",
    "$\\newcommand{\\mat}[1]{\\left[\\begin{array}#1\\end{array}\\right]}$\n",
    "$\\newcommand{\\vx}{{\\mathbf x}}$\n",
    "$\\newcommand{\\hx}{\\hat{\\mathbf x}}$\n",
    "$\\newcommand{\\vbt}{{\\mathbf\\beta}}$\n",
    "$\\newcommand{\\vy}{{\\mathbf y}}$\n",
    "$\\newcommand{\\vz}{{\\mathbf z}}$\n",
    "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
    "$\\newcommand{\\vu}{{\\mathbf u}}$\n",
    "$\\newcommand{\\vv}{{\\mathbf v}}$\n",
    "$\\newcommand{\\vw}{{\\mathbf w}}$\n",
    "$\\newcommand{\\col}{{\\operatorname{Col}}}$\n",
    "$\\newcommand{\\nul}{{\\operatorname{Nul}}}$\n",
    "$\\newcommand{\\vb}{{\\mathbf b}}$\n",
    "$\\newcommand{\\va}{{\\mathbf a}}$\n",
    "$\\newcommand{\\ve}{{\\mathbf e}}$\n",
    "$\\newcommand{\\setb}{{\\mathcal{B}}}$\n",
    "$\\newcommand{\\rank}{{\\operatorname{rank}}}$\n",
    "$\\newcommand{\\vp}{{\\mathbf p}}$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\\newcommand{\\mat}[1]{\\left[\\begin{array}#1\\end{array}\\right]}\n",
    "\\newcommand{\\vx}{{\\mathbf x}}\n",
    "\\newcommand{\\hx}{\\hat{\\mathbf x}}\n",
    "\\newcommand{\\vbt}{{\\mathbf\\beta}}\n",
    "\\newcommand{\\vy}{{\\mathbf y}}\n",
    "\\newcommand{\\vz}{{\\mathbf z}}\n",
    "\\newcommand{\\vb}{{\\mathbf b}}\n",
    "\\newcommand{\\vu}{{\\mathbf u}}\n",
    "\\newcommand{\\vv}{{\\mathbf v}}\n",
    "\\newcommand{\\vw}{{\\mathbf w}}\n",
    "\\newcommand{\\va}{{\\mathbf a}}\n",
    "\\newcommand{\\ve}{{\\mathbf e}}\n",
    "\\newcommand{\\vp}{{\\mathbf p}}\n",
    "\\newcommand{\\R}{{\\mathbb{R}}}\n",
    "\\newcommand{\\col}{{\\operatorname{Col}}}\n",
    "\\newcommand{\\nul}{{\\operatorname{Nul}}}\n",
    "\\newcommand{\\rank}{{\\operatorname{rank}}}\n",
    "\\newcommand{\\setb}{{\\mathcal{B}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We consider a number of concepts and corresponding definitions from linear algebra that will allow us to embed data entries in a vector space. This in turn will allow us to use common constructs to compare data entries with one another.\n",
    "\n",
    "Note that the kinds of data we can embed in a vector space is varied, and can include:\n",
    "* Records describing (human) users\n",
    "* Graphs\n",
    "* Images\n",
    "* Videos\n",
    "* Text (e.g., webpages, books, etc.)\n",
    "* Strings (e.g., DNA sequences)\n",
    "* Timeseries\n",
    "\n",
    "Once we convert these to vectors, we can use the same constructs to work with any of these different types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A **data set** is a collection of zero or more **data entries**. If a data set is represented as a table, these would correspond to the **rows** of the table.\n",
    "\n",
    "Each data entry usually consists of a number of **attributes** (also known as **dimensions** or **features**). If a data set is represented as a table, these would correspond to the **columns**.\n",
    "\n",
    "For example, the following data set consists of two data entries. Each entry has three attribtues: a name, an age, and an income.\n",
    "\n",
    "* (`\"J. Smith\"`, 25, \\$200,000)\n",
    "* (`\"M. Jones\"`, 47, \\$45,000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If all the features of the data entries within a data set can be represented as real numbers, then we can visualize each data object as a point in a vector space. For example:\n",
    " * (25, USD 200,000) $\\rightarrow \\mat{{c}25\\\\200000}$.\n",
    "\n",
    "Alternatively, if all the features are binary then we can think of each data object as a binary vector in a vector space.\n",
    "\n",
    "The vector space that contains the vectors corresponding to data entries is often called a **feature space.** Attributes could then be called **feature dimensions** of this space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Definition:** A (real-valued) **vector** is an ordered, finite tuple of real numbers $(x_1, ..., x_n) \\in \\R^n$.\n",
    "\n",
    "Note that we will use individual variables to denote an entire vector (e.g., $x \\in \\R^n$), and that same variable with a subscript index (e.g., $x_i$) to denote individual components of that vector; thus, we have that $x = (x_1, ..., x_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metrics and Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A **metric** over a vector space $V$ is a function $m:V \\times V \\to \\R$ that maps pairs of vectors to a real number and satisfies the following conditions for all $x,y,z \\in V$:\n",
    "* **Non-negativity:** $m(x, y) \\geq 0$ \n",
    "* **Identity of indiscernibles:** $m(x, y) = 0$ iff $x = y$\n",
    "* **Symmetry:** $m(x, y) = m(y, x)$\n",
    "* **Triangle inequality:** $m(x, z) \\leq m(x, y) + m(y, z)$\n",
    "\n",
    "Metrics can be viewed as providing a notion of distance between vectors in the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Metrics provide a natural way to quantify how **similar** or **dissimilar** two objects are. One way to interpret the real value computed by a metric function is as a measure of dissimilarity (i.e., the larger the distance is between two vectors, the more dissimilar are the two data entries corresponding to those two vectors).\n",
    "\n",
    "Sometimes we will use \"distance\" informally when referring to a dissimilarity function, even if we are not sure it is a metric according to the definition above. We will try to say \"dissimilarity\" in those cases, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why is it important or valuable for a dissimilarity to be a metric?\n",
    "\n",
    "The additional constraints allow us to reason about and more easily visualize the data. The main way this happens is through the triangle inequality. We can interpret it in the following way: if two objects are \"similar\" to another object, then they are \"similar\" to each other. \n",
    "\n",
    "This does not always apply to all data sets, however. For example, distances in a network or graph may not obey the triangle inequality. For another example, consider fuel consumption on rough terrain: going uphill from opposite points of a hill may consume some amount of fuel, but going uphill and then downhill to the other side would consume less fuel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Definitions of distance or dissimilarity are usually different for real, boolean, categorical, and ordinal feature dimensions. Furthermore, weights may be associated with different feature dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix Representations and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When treating data sets as vectors in a vector space, it is often convenient to manage that data in matrix form. The standard way of doing so is illustrated below, wherein feature dimensions correspond to matrix columns and data entries correspond to rows:\n",
    "\n",
    "$$ \\mbox{$m$ data entries}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{ccccc}\n",
    "\\begin{array}{c}x_{11}\\\\\\vdots\\\\x_{i1}\\\\\\vdots\\\\x_{m1}\\end{array}&\n",
    "\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n",
    "\\begin{array}{c}x_{1j}\\\\\\vdots\\\\x_{ij}\\\\\\vdots\\\\x_{mj}\\end{array}&\n",
    "\\begin{array}{c}\\dots\\\\\\ddots\\\\\\dots\\\\\\ddots\\\\\\dots\\end{array}&\n",
    "\\begin{array}{c}x_{1n}\\\\\\vdots\\\\x_{in}\\\\\\vdots\\\\x_{mn}\\end{array}\n",
    "\\end{array}\\right]}^{\\mbox{$n$ features}} $$\n",
    "\n",
    "As a convention, we will use $m$ to refer to the number of rows (i.e., data entries) and $n$ for the number of columns (i.e., the number of feature dimensions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another common scenario involving a matrix representation is one in which we consider the distances between all pairs of data entries. The matrix representation then looks as below:\n",
    "\n",
    "$$ \\mbox{$m$ data entries}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\n",
    "\\overbrace{\\left[\\begin{array}{cccc}\n",
    "\\begin{array}{c}0\\\\d(1,2)\\\\d(1,3)\\\\\\vdots\\\\d(1,m)\\end{array} &\n",
    "\\begin{array}{c}\\;\\\\0\\\\d(2,3)\\\\\\vdots\\\\d(2,m)\\end{array} &\n",
    "\\begin{array}{c}\\;\\\\\\;\\\\\\ddots\\\\\\vdots\\\\\\dots\\end{array} &\n",
    "\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\0\\end{array} \\\\\n",
    "\\end{array}\\right]}^{\\mbox{$m$ data entries}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Useful Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are many functions over vector spaces that satisfy the definition of a metric. One general form is the **Minkowski distance**: for $p \\geq 1$, it is defined as:\n",
    "\n",
    "$$ L_p(x, y) = \\left(\\sum_{i=1}^d |x_i - y_i|^p\\right)^{\\frac{1}{p}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A special case of this where $p=2$ yields the familiar **Euclidean** metric:\n",
    "\n",
    "$$ L_2(x, y) = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def L2(x, y):    \n",
    "    return (sum((x[i] - y[i])**2 for i in range(min(len(x),len(y)))))**0.5\n",
    "\n",
    "def L2(x, y):\n",
    "    return (sum((xi - yi)**2 for (xi,yi) in zip(x,y)))**0.5\n",
    "\n",
    "print(L2((1,2,3), (4,5,6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the $p = 1$ case we get the **Manhattan** or **taxicab** metric:\n",
    "\n",
    "$$L_1(x, y) = \\sum_{i=1} |x_i - y_i|$$\n",
    "\n",
    "When working with boolean or binary discrete vector spaces, this also corresponds to the **Hamming** distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we take the limit as $p$ gets large we get the metric based on the $\\ell_\\infty$ norm, which yields the __largest element__ in the difference between two vectors (i.e., the dimension along which the difference between two vectors will yield the longest projection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another related idea is the case where $p = 0$. In this case, the function counts the number of __nonzero__ elements in the difference vector between two vectors, which also corresponds to the number of feature dimensions along which two vectors differ. This is known as the difference vector's __sparsity.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Similarity and Dissimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Definition:** Given two vectors $x, y \\in \\R^n$, the dot product of these two vectors is defined as:\n",
    "\n",
    "$$x \\cdot y \\ = \\ \\sum_{i=1}^{n} x_i \\cdot y_i \\ = \\ x_1 \\cdot x_2 + ... + x_n \\cdot y_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The dot product of two vectors can be used to compute the __cosine of the angle__ between the two vectors:\n",
    "\n",
    "$$\\cos(x, y) = \\frac{x \\cdot y}{\\Vert x \\Vert \\cdot \\Vert y \\Vert} = \\frac{x}{\\Vert x \\Vert} \\cdot \\frac{y}{\\Vert y \\Vert}$$\n",
    "\n",
    "Note that this value is **large** when $x \\approx y$. Thus, it is a __similarity__ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We often find that we have a similarity function $s$, but need to convert it to a dissimilarity function $d$. Two straightforward ways of doing so are (for some properly chosen $k$):\n",
    "* $d(x,y) = \\frac{1}{s(x,y)}$\n",
    "* $d(x,y) = k - s(x,y)$\n",
    "\n",
    "For cosine similarity, we often use:\n",
    "\n",
    "$$d(x, y) = 1 - \\cos(x, y)$$\n",
    "\n",
    "Note that this is **not a metric**! However, if we recover the actual angle beween $x$ and $y$, that is a metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bit Vectors and Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When working with bit vectors, the $p=1$ case is commonly used and called the **Hamming** distance. This has a natural interpretation: it quantifies how well two bit vectors match. It can also be viewed as the **edit distance**: what is the smallest number of bit flips that will convert one vector into the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In other cases, the Hamming distance is not a very appropriate metric. Consider the case in which the bit vector is being used to represent a set. In that case, Hamming distance measures the **size of the set difference**.\n",
    "\n",
    "For example, consider two documents.   We will use bit vectors to represent the sets of words in each document. Consider the following two cases:\n",
    "* **Case 1**: both documents are large and almost identical, but differ in 10 words\n",
    "* **Case 2**: both documents are small and disjoint, having 5 words each that do not occur in the other document\n",
    "\n",
    "Case 1 would yield a larger Hamming distance than Case 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What matters is not just the size of the set difference, but the size of the intersection as well. This leads to the **Jaccard** similarity function for two sets $A$ and $B$:\n",
    "\n",
    "$$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "This yields values in the range between 0 and 1, so a natural dissimilarity metric that can be derived from this is $1 - J(A, B)$.\n",
    "\n",
    "$$J_d(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Two example documents.\n",
    "d = \"Azertyuiop won a hurdle at Auteuil, Paris in October.\"\n",
    "e = \"October in Paris is marked with cool weather.\"\n",
    "\n",
    "# Normalize the content.\n",
    "d = d.lower().replace(\",\",\"\").replace(\".\",\"\").split(\" \")\n",
    "e = e.lower().replace(\",\",\"\").replace(\".\",\"\").split(\" \")\n",
    "\n",
    "X = set(d)\n",
    "Y = set(e)\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def J(A, B):\n",
    "    return len(A & B) / len(A | B)\n",
    "\n",
    "print(J(X, Y))\n",
    "print(J(X, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "words = sorted(list(X | Y))\n",
    "print(words)\n",
    "\n",
    "x = tuple(1 if word in X else 0 for word in words)\n",
    "y = tuple(1 if word in Y else 0 for word in words)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "def L1(x, y):\n",
    "    return sum(abs(x[i] - y[i]) for i in range(0,min(len(x), len(y))))\n",
    "\n",
    "def L1(x, y):\n",
    "    return sum(abs(xi-yi) for (xi,yi) in zip(x,y))\n",
    "\n",
    "print(L1(x,y))\n",
    "print(1 - (L1(x,y) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A time series is an ordered sequence of real numbers, representing the measurements of a real variable at equal time intervals. Examples of real-world phenomena that can be represented in this way include:\n",
    "* stock prices\n",
    "* volume of sales over time\n",
    "* daily temperature readings\n",
    "\n",
    "A time series database is a large collection of such time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Similarity of Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How can we measure the \"similarity\" of two time series? We will assume they are the same length. Examples of problems that can be viewed this way include:\n",
    "* finding companies with similar stock price movements over a time interval\n",
    "* finding users with similar credit usage patterns\n",
    "\n",
    "Other types of data are amenable to similar sorts of analyses:\n",
    "* find similar DNA sequences (where \"time\" corresponds to their arrangement in the genome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We face two problems:\n",
    "1. defining a meaningful similarity (or distance) function\n",
    "2. finding an efficient algorithm to compute it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Issues with Similarity Measures from Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We could just view each sequence as a vector and use a metric to measure the similarity. The advantages are:\n",
    "1. it is easy to compute - linear in the length of the time series (i.e., $O(n)$)\n",
    "2. it is a metric and thus possesses all the associated properties\n",
    "\n",
    "There is, however, a disadvantage: what is the meaning of the metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "display(Image(\"figs/L5-ts-euclidean.png\", width=550))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the above, it is intuitive that that `ts1` and `ts2` are \"more similar\". However, according to Euclidean distance: d(ts1, ts2) = 26.9, while d(ts1, ts3) = 23.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general, there may be different aspects of a time series that are important in different settings. One approach is to decide what is important about time series in the particular application domain in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is an example of **feature engineering**: the process of identifying, defining, and computing some **derived** attributes from your data entries that make its important properties usable in a subsequent step. \n",
    "\n",
    "Thus, a typical approach that employs this technique may involve:\n",
    "* identifying relevant features (perhaps algorithmically using other techniques)\n",
    "* extracting the relevant features (in some automated way)\n",
    "* using an existing method (e.g., metrics) to define similarity over the new, derived features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the time series application above, there may be a number of potential techniques that could be used for the feature engineering step, including:\n",
    "* Fourier coefficients (to capture periodicity)\n",
    "* Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Feature engineering can become fairly sophisticated, and could even require the use of metrics. One example is \"bump hunting\" within time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "display(Image(\"figs/L5-DTW-1.png\", width=550))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that both of the above time series have the same characteristics: four bumps. However, matching the entries of the vectors according to their position will not capture this, and metric-based notions of similarity will indicate that the sequences are very different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One solution to this is called **dynamic time warping**, or **sequence alignment**. The idea is to allow acceleration or deceleration of signals along the time dimension. Classic applications of this approach include:\n",
    "* speech recognition\n",
    "* signature (i.e., pattern) recognition\n",
    "\n",
    "Consider two sequences $X = x_1, x_2, \\dots, x_n$ and $Y = y_1, y_2, \\dots, y_n$. We are allowed to extend each sequence by repeating any elements that we choose to form two new sequences $X'$ and $Y'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is a way to visualize this algorithm. Consider a matrix $M$ where $M_{ij} = |x_i - y_j|$ (or some other error measure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "display(Image(\"figs/L5-DTW-2.png\", width=550))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$M$ represents the amount of error we have if we match $x_i$ with $y_j$. So we seek a **path through $M$ that minimizes the total error**. In the diagram above, we start in the lower left and work our way up via a continuous path. There are some restrictions (we choose these to simplify the algorithm and make the problem more tractable): \n",
    "* **monotonicity**: path should not go down or to the left\n",
    "* **continuity**: no elements may be skipped in sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This can be solved via dynamic programming.  However, the algorithm is still quadratic in $n$. Hence, we may choose to put a restriction on the amount that the path can deviate from the diagonal. The basic algorithm could have the structure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "`D[0, 0] = 0\n",
    "for i in range(n):\n",
    "    for j in range (m):\n",
    "        D[i,j] = M[i,j] + min( D[i-1, j],  # insertion\n",
    "                               D[i, j-1],  # deletion\n",
    "                               D[i-1, j-1]) # match`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Unfortunately, the algorithm is still *quadratic* in $n$. More precisely, it runs in time $O(nm)$ on sequences of length $n$ and $m$. Hence, we may choose to put a restriction on the amount that the path can deviate from the diagonal. This is implemented by not allowing the path to pass through locations where $|i - j| > w$. Then the algorithm has complexity $O(nw)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Related Notions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A closely related idea concerns strings. Strings are just sequences, like time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given two strings, one way to define a 'distance' between them as the minimum number of edit operations that are needed to transform one string into the other. The edit operations are insertion, deletion, and substitution of single characters. This is called __edit distance__ or __Levenshtein distance.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A very similar dynamic programming algorithm can be used to find this distance. In bioinformatics this algorithm is called \"Smith-Waterman\" sequence alignment. At this point, there are much more sophisticated similarity measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How are (dis)similarity measures used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Similarity measures are most useful when you have a large collection of objects. Computing their pairwise similarities (or distances) is often used the first step in a more complex analysis. Subsequent steps might involve:\n",
    "* clustering\n",
    "* a database allowing to search for similar objects\n",
    "* fitting a model for how the objects were generated"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
